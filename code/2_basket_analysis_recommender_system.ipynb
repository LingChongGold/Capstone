{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basket Analysis and Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform basket analysis using association rules and build a recommender system using the implicit library. Explaination on the individual steps, how the libraries work and evaluation of the algorithm and model will be made in their respective sections.\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries and Load Dataset](#Import-Libraries-and-Load-Dataset)\n",
    "- [Apriori - Basket Analysis](#Apriori---Basket-Analysis)\n",
    "- [Recommender System](#Recommender-System)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import implicit\n",
    "import pickle\n",
    "\n",
    "from apyori import apriori\n",
    "from collections import defaultdict\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../dataset/cleaned/combined_cleansed.csv')\n",
    "validation_set = pd.read_csv('../dataset/cleaned/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori - Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association rules captures pattern of items appearing frequently together. For example, we have a total of 10 orders containing varying items, out of the 10 orders, diapers and beer are bought together in 4 of them. The algorithm will detect such patterns and calculate the lift and confidence of beer and diaper. More detailed explaination on confidence and lift is provided below:\n",
    "    \n",
    "- Confidence = The likelihood that item B is bought when item A is bought\n",
    "    - I have 100 transactions where bread is bought, 20 of them contains both bread and jam, Confidence of bread→jam = 20/100 = 0.2 = 20%\n",
    "    - The likelihood of buying jam when bread is purchased is 20%\n",
    "\n",
    "- Lift = the increase in ratio of the sale of B when A is sold, it can be calculated by (Confidence A→B) / (Support B).\n",
    "    - A higher lift means that the likelihood of the products being bought together is higher\n",
    "    - A lift lesser than 1 means that the items are not likely to be bought together\n",
    "    - A lift equals to 1 means that there are no association between both products\n",
    "    - Lift(bread → jam) = (20/100) / (10/100) = 2\n",
    "    - The likelihood of buying jam and bread together is 2 times more likely than just buying bread alone\n",
    "\n",
    "Some use cases on how basket analysis can be assist business:\n",
    "- X can be recommended if Y is present in cart when customer is checking out \n",
    "- X and Y could be combined into a new product, such as having Y in flavors of X.\n",
    "- Both X and Y can be placed on the same shelf, so that buyers of one item would be prompted to buy the other.\n",
    "- Promotional discounts could be applied to just one out of the two items.\n",
    "- Advertisements on X could be targeted at buyers who purchase Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Dataset for Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association rules utilises a nested list, we will first prepare our dataset by incorporating all items purchased in one order in a list, which will be nested into a master list containing all orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(x):\n",
    "    prod_dict[x[0]].append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[['order_id','product_name']].apply(add_to_dict, axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_list = list(prod_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving prepared data\n",
    "\n",
    "with open('../pickles/purchase_list.data', 'wb') as filehandle:\n",
    "    pickle.dump(purchase_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_list = [x for x in purchase_list if len(x) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will only keep orders with more than 1 item\n",
    "- Transaction with only 1 item will increase our total transaction count and will not be purposeful to capture patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm takes in 4 parameters, an explaination of the parameters chosen for the algorithm below:\n",
    "- min_support\n",
    "    - To only include items that appeared in at least 3000 times out of our total transaction\n",
    "- min_confidence\n",
    "    - To only include items which are bought together for at least 20% out of the total transactions where only the second item is bought\n",
    "- min_lift \n",
    "    - To only include rules where it is minimally 2 times more to purchse both 2 items compared to purchasing only the first item\n",
    "- min length\n",
    "    - I want at least 2 products in all the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apriori(purchase_list, min_support=3000/len(purchase_list), min_confidence=0.2, min_lift=2, min_length=2)\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "\n",
    "with open('../pickles/association_rules_01_percent_min_support.data', 'wb') as filehandle:\n",
    "    pickle.dump(association_results, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(association_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 421 associations of purchases after filtering. We will convert them into a dataframe and explore the rules which the algorithm had determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for item in association_results:\n",
    "\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    value0 = str(items[0])\n",
    "    value1 = str(items[1])\n",
    "    \n",
    "    # second index for the inner listing\n",
    "    value2 = str(item[1])[:7]\n",
    "    \n",
    "    value3 = str(item[2][0][2])[:7]\n",
    "    value4 = str(item[2][0][3])[:7]\n",
    "    \n",
    "    rows = (value0, value1, value2, value3, value4)\n",
    "    result.append(rows)\n",
    "\n",
    "labels = ['Antecedent', 'Consequents', 'Support', 'Confidence', 'Lift']\n",
    "product_suggestions = pd.DataFrame(result, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedent</th>\n",
       "      <th>Consequents</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Honeycrisp Organic</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>0.00774</td>\n",
       "      <td>0.27941</td>\n",
       "      <td>2.26811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>Apples</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>0.25411</td>\n",
       "      <td>2.06274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clementines</td>\n",
       "      <td>Apples</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.32820</td>\n",
       "      <td>33.6140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asparation/Broccolini/Baby Broccoli</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0.00198</td>\n",
       "      <td>0.36813</td>\n",
       "      <td>2.39230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baby Cucumbers</td>\n",
       "      <td>Hass Avocados</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.22707</td>\n",
       "      <td>14.0823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Antecedent             Consequents  Support  \\\n",
       "0             Apple Honeycrisp Organic  Bag of Organic Bananas  0.00774   \n",
       "1               Bag of Organic Bananas                  Apples  0.00101   \n",
       "2                          Clementines                  Apples  0.00131   \n",
       "3  Asparation/Broccolini/Baby Broccoli                  Banana  0.00198   \n",
       "4                       Baby Cucumbers           Hass Avocados  0.00103   \n",
       "\n",
       "  Confidence     Lift  \n",
       "0    0.27941  2.26811  \n",
       "1    0.25411  2.06274  \n",
       "2    0.32820  33.6140  \n",
       "3    0.36813  2.39230  \n",
       "4    0.22707  14.0823  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_suggestions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clementines appeared in 0.00131 of the total transactions (0.13%)\n",
    "- The likelihood of someone buying Apples when they purchase Clementines is 0.32 (32%)\n",
    "- Past transanctions shows that people are 33 times more likely to buy Apples and Clementines compared to just buying Apples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Data\n",
    "In this project, we are purely leveraging on implicit data which is gathered through customer's behaviour. In this case we got these data from the purchasing behavour, what items the user had purchased and how many times they had purchased the item. Using implicit data yield some advantage over explicit data.\n",
    "- Getting explicit ratings and reviews may not always be easy as it requires additional actions by the customer\n",
    "- Explicit ratings and reviews provided by customer can be skewed to certain level of biasness based on the situation the rating is provided and cultural habits\n",
    "    - Ratings and reviews can be dependent on the user's mood\n",
    "    - A user's 4 star rating can be equivalent another user's 5 star rating due to cultural differences\n",
    "\n",
    "#### Recommender System\n",
    "Interaction between customer and item is a basis of how our recommender system works. An absence of interaction could mean that the customer do not like the item or more often, the customer do not know about the item yet.\n",
    "A good recommender system is able to identify hidden features a user like based on their past bahaviour and behaviour of similar users and matching them with products that has these hidden features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be utilising the Alternating Least Squares model to fit our data and find similarities. For ALS, we need to utilise matrix factorization, matrix factorization is taking a large matrix and factor it into some smaller representation of the original. \n",
    "\n",
    "A problem in collaborative filtering for recommender systems is that our original matrix have millions of different dimensions, but our tastes may not be so complex. For example, I could have bought hundreds of different products, but these products may only represent a few different tastes.\n",
    "\n",
    "Using matrix factorization, we can mathematically reduce the dimensionality of our original all users by all items matrix into a smaller all items and what tastes they represent vector and each user and their taste value vector. These tastes are latent or hidden features which we learn them from our data. This reduction makes it much more computationally efficient and also gives us better results as we can reason items in a more compact taste space.\n",
    "\n",
    "We convert our data into a user by product sparse matrix and their taste values will be the number of purchase a user made for the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_df = full_df.groupby(['user_id', 'product_name', 'product_id'])['product_id'].agg('count').to_frame('purchase_count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of unique users\n",
    "users = list(np.sort(collaborative_df['user_id'].unique()))\n",
    "# get a list of unique products\n",
    "products = list(collaborative_df['product_id'].unique())\n",
    "# get a list of purchase count\n",
    "purchase_count = list(collaborative_df['purchase_count'])\n",
    "\n",
    "# get the row indices\n",
    "cols = collaborative_df['user_id'].astype('category', CategoricalDtype(categories = users)).cat.codes\n",
    "# get the column indices\n",
    "rows = collaborative_df['product_id'].astype('category', CategoricalDtype(categories = products)).cat.codes\n",
    "\n",
    "collaborative_sparse = sparse.csr_matrix((purchase_count, (rows, cols)), shape = (len(products), len(users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS is an iterative optimization process where we try to arrive closer to a factorized representation of our original data with each iteration.\n",
    "\n",
    "We have our original matrix of size users * products and the feedback data is the purchase count of the product. The original matrix is then turned into one matrix consisting of users and hidden features and another with items and hidden features. In these 2 matrices we have weights for how each user and product relates to each hidden feature. We calculate these 2 matrices so that their product is as close as the original matrix as possible.\n",
    "\n",
    "The model merge the preference a user have for an item with the confidence level we have for that preference. We start out with representing missing values as a negative preference with a low confidence value while existing values have a positive preference with a high confidence value. \n",
    "\n",
    "Preference is a binary representation derived from feedback data, purchase count. If the user had purchased the item, it is set to 1, else it will be set to 0\n",
    "\n",
    "Confidence is calculated using the magnitude of the feedback data, we will have a larger confidence the more times a the customer purchased the item. The rate of which our confidence increases is set through a linear scaling factor alpha. This means that there is only one interaction between a user and the item the confidence will be higher than that of the user with an item which was not purchased before given the alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f1bfbb0f7b4df8a122dc6a9e1e03fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# closest to the power point\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_and_validation_orders(df, model, fitted, user):\n",
    "    recommendations = model.recommend(user, fitted, N = 4, filter_already_liked_items = True)\n",
    "    product_dict = dict(zip(full_df.product_id, full_df.product_name))\n",
    "    \n",
    "    print('Recommended items for User {} are: \\n'.format(user))\n",
    "    for i in recommendations:\n",
    "        print(i[0], product_dict.get(i[0]), i[1])\n",
    "    print('===========================================================')\n",
    "    print('User {} validation transactions are:\\n'.format(user))\n",
    "    print(validation_set[validation_set['user_id'] == user][['product_name', 'user_id']].to_string(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for User 1 are: \n",
      "\n",
      "13517 Whole Wheat Bread 1.1968588\n",
      "20063 Hazelnuts in Milk Chocolate, 33% Cocoa 1.1774435\n",
      "26853 Complete Wheat 100% Whole Wheat Bread 1.1398611\n",
      "15487 Raspberry English Tea Scones 1.1145719\n",
      "===========================================================\n",
      "User 1 validation transactions are:\n",
      "\n",
      "                     product_name  user_id\n",
      "                             Soda        1\n",
      "            Organic String Cheese        1\n",
      "         0% Greek Strained Yogurt        1\n",
      " XL Pick-A-Size Paper Towel Rolls        1\n",
      "           Milk Chocolate Almonds        1\n",
      "                       Pistachios        1\n",
      "            Cinnamon Toast Crunch        1\n",
      "       Aged White Cheddar Popcorn        1\n",
      "               Organic Whole Milk        1\n",
      "              Organic Half & Half        1\n",
      "                Zero Calorie Cola        1\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_and_validation_orders(collaborative_sparse, model, user_items, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relevant Recommendations__\n",
    "- Recommended Hazelnut in Milk Chocolate\n",
    "- Purchased Milk Chocolate Almonds\n",
    "\n",
    "__Room for Improvement__\n",
    "- Recommended bread twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for User 58144 are: \n",
      "\n",
      "34172 Top Ramen Shrimp Flavor Instant Noodle Soup 1.1561155\n",
      "39322 Caramel Almond and Sea Salt Nut Bar 1.1357532\n",
      "35175 Mini Stuffers Hamburger Dill Chips 1.1336474\n",
      "19604 Medium Scarlet Raspberries 1.0747831\n",
      "===========================================================\n",
      "User 58144 validation transactions are:\n",
      "\n",
      "                                      product_name  user_id\n",
      "                        Electrolyte Enhanced Water    58144\n",
      "                                            Banana    58144\n",
      " Air Chilled Organic Boneless Skinless Chicken ...    58144\n",
      "                              Lime Sparkling Water    58144\n",
      "                          Non Fat Raspberry Yogurt    58144\n",
      "                                   Farfalle No. 93    58144\n",
      "                Total 0% Nonfat Plain Greek Yogurt    58144\n",
      "                             Original Orange Juice    58144\n",
      "                     Best Sloppy Joe Skillet Sauce    58144\n",
      "                       Organic Cauliflower Florets    58144\n",
      "                        Grated Parmigiano Reggiano    58144\n",
      "                                     Whole Almonds    58144\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_and_validation_orders(collaborative_sparse, model, user_items, 58144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relevant Recommendations__<br>\n",
    "- Recommended Medium Scarlet Raspberries\n",
    "- Purchased Banana\n",
    "- Purchased Raspberry Yoghurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for User 114401 are: \n",
      "\n",
      "44898 Organic Mac And Trees Fun Shape Macaroni & Cheese 1.1370347\n",
      "35488 Organic Dry Roasted Premium Flaxseed 1.1338583\n",
      "2190 Spicy Red Lentil Sauce 1.1187676\n",
      "21702 Puna Coconut Pineapple 1.1060191\n",
      "===========================================================\n",
      "User 114401 validation transactions are:\n",
      "\n",
      "                                  product_name  user_id\n",
      "                                    Whole Milk   114401\n",
      " No Pulp Calcium & Vitamin D Pure Orange Juice   114401\n",
      "                 Original Fresh Stack Crackers   114401\n",
      "                         Cheddar Broccoli Rice   114401\n",
      "                              Corn Pops Cereal   114401\n",
      "                       Eggo Strawberry Waffles   114401\n",
      "       Original 100% Pure No Pulp Orange Juice   114401\n",
      "                            Orange Juice To-Go   114401\n",
      "                 All Natural Peach Tea Bottles   114401\n",
      "                          Hickory Smoked Bacon   114401\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_and_validation_orders(collaborative_sparse, model, user_items, 114401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relevant Recommendations__<br>\n",
    "- Recommended Puna Chocolate Pineapple (Fuit Juice)\n",
    "- Purchased fruit juices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for User 3754 are: \n",
      "\n",
      "33502 Double Cheese Baked Snack Mix 1.1659267\n",
      "45339 Men's Refresh Dandruff Shampoo 1.060647\n",
      "29642 Ultra Soft Bath Tissue 1.0497061\n",
      "13810 Reclosable Gallon Freezer Bags 1.0313741\n",
      "===========================================================\n",
      "User 3754 validation transactions are:\n",
      "\n",
      "                                      product_name  user_id\n",
      "                              Twice Baked Potatoes     3754\n",
      "                            Whipped Sweet Potatoes     3754\n",
      " 100% Natural Skin & Hair Revitalizing Coconut Oil     3754\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_and_validation_orders(collaborative_sparse, model, user_items, 3754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relevant Recommendations__<br>\n",
    "- Recommended Shampoo\n",
    "- Purchased Skin and Hair products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for User 200372 are: \n",
      "\n",
      "30890 MCT Oil 1.26377\n",
      "8651 Shipping Packaging Tape Heavy Duty 1.2522316\n",
      "17419 Sprouted Whole Wheat Bread 1.209813\n",
      "17018 Ghee Vanilla Bean 1.1393975\n",
      "===========================================================\n",
      "User 200372 validation transactions are:\n",
      "\n",
      "                product_name  user_id\n",
      "                   Diet Cola   200372\n",
      "       Original Potato Chips   200372\n",
      "  Salsa Con Queso Medium Dip   200372\n",
      "        Pure Sport Body Wash   200372\n",
      "          Snickers Ice Cream   200372\n",
      " Raspberry Cheesecake Gelato   200372\n",
      "                    Rosemary   200372\n",
      "                Red Potatoes   200372\n",
      "   2% Low Fat Cottage Cheese   200372\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_and_validation_orders(collaborative_sparse, model, user_items, 200372)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relevant Recommendations__<br>\n",
    "- None\n",
    "\n",
    "__Possible Reasons for Recommendation__<br>\n",
    "- Recommended Ghee Vanilla Bean (a kind of bread spread)\n",
    "- Previous orders includes a variety of spreads\n",
    "<br><br>\n",
    "- Recommended MCT Oil (Suppliment for weight loss or energy)\n",
    "- Previous orders includes supplements and energy drinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps and Future Improvements\n",
    "- Deploy the recommender system and evaluate the effectiveness based on the take up rate of recommended items and evaluate the results\n",
    "\n",
    "- Add features tags to the products like organic, natural, convenient, fresh, price range, manufacturer for the next iteration of improvement\n",
    "\n",
    "- Use neural networks, Sequential to predict customer's next purchase and perform association rules for new recommendations\n",
    "\n",
    "- Additional analysis can be performed on the customer with more data to gain insights on additional segments based on their recency of their last purchase, frequency of order, amount spent in the store."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
