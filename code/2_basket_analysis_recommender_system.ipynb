{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import implicit\n",
    "import pickle\n",
    "\n",
    "from apyori import apriori\n",
    "from collections import defaultdict\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../dataset/cleaned/combined_cleansed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori - Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both X and Y can be placed on the same shelf, so that buyers of one item would be prompted to buy the other.\n",
    "- Promotional discounts could be applied to just one out of the two items.\n",
    "- Advertisements on X could be targeted at buyers who purchase Y.\n",
    "- X and Y could be combined into a new product, such as having Y in flavors of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(x):\n",
    "    prod_dict[x[0]].append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[['order_id','product_name']].apply(add_to_dict, axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_list = list(prod_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/purchase_list.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(purchase_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/purchase_list.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    purchase_list = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3214669"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(purchase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_list = [x for x in purchase_list if len(x) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only keep orders which have more than 1 orders\n",
    "- transaction with only 1 order will increase our total transaction count and will not be useful for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3057605"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(purchase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- support = I want items that had been bought in at least 3000 times\n",
    "- confidence = I want at least 20% of the transactions where the items are bought together when compared to the transactions when only the second item is bought\n",
    "- lift = 2\n",
    "- min length = I want at least 2 products in our rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apriori(purchase_list, min_support=3000/len(purchase_list), min_confidence=0.2, min_lift=2, min_length=2)\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/association_rules_01_percent_min_support.data', 'wb') as filehandle:\n",
    "    pickle.dump(association_results, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/association_rules_01_percent_min_support.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    association_results = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(association_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'Apple Honeycrisp Organic', 'Bag of Organic Bananas'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_results[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support is the number of transactions that contain the item divide by the total number of transactions\n",
    "    - I have 100 transactions, 10 of them contains jam = 10/100 = 0.1 = 10%\n",
    "    \n",
    "- Confidence = The likelihood that item B is bought when item A is bought\n",
    "    - I have 100 transactions where only bread is bought, 20 of them contains both bread and jam, Confidence of bread→jam = 20/100 = 0.2 = 20%\n",
    "    - The likelihood of buying jam when bread is purchased is 20%\n",
    "\n",
    "- Lift = the increase in ratio of the sale of B when A is sold, it can be calculated by (Confidence A→B) / (Support B).\n",
    "    - A higher lift means that the likelihood of the products being bought together is higher\n",
    "    - A lift lesser than 1 means that the items are not likely to be bought together\n",
    "    - A lift equals to 1 means that there are no association between both products\n",
    "    - Lift(bread → jam) = (20/100) / (10/100) = 2\n",
    "    - The likelihood of buying jam and bread together is 2 times more likely than just buying bread alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for item in association_results:\n",
    "\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    value0 = str(items[0])\n",
    "    value1 = str(items[1])\n",
    "    \n",
    "    # second index for the inner listing\n",
    "    value2 = str(item[1])[:7]\n",
    "    \n",
    "    value3 = str(item[2][0][2])[:7]\n",
    "    value4 = str(item[2][0][3])[:7]\n",
    "    \n",
    "    rows = (value0, value1, value2, value3, value4)\n",
    "    result.append(rows)\n",
    "\n",
    "labels = ['Antecedent', 'Consequents', 'Support', 'Confidence', 'Lift']\n",
    "product_suggestions = pd.DataFrame(result, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_suggestions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedent</th>\n",
       "      <th>Consequents</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Honeycrisp Organic</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>0.00774</td>\n",
       "      <td>0.27941</td>\n",
       "      <td>2.26811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apples</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>0.25411</td>\n",
       "      <td>2.06274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apples</td>\n",
       "      <td>Clementines</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.32820</td>\n",
       "      <td>33.6140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Asparation/Broccolini/Baby Broccoli</td>\n",
       "      <td>0.00198</td>\n",
       "      <td>0.36813</td>\n",
       "      <td>2.39230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baby Cucumbers</td>\n",
       "      <td>Hass Avocados</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.22707</td>\n",
       "      <td>14.0823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Antecedent                          Consequents  Support  \\\n",
       "0  Apple Honeycrisp Organic               Bag of Organic Bananas  0.00774   \n",
       "1                    Apples               Bag of Organic Bananas  0.00101   \n",
       "2                    Apples                          Clementines  0.00131   \n",
       "3                    Banana  Asparation/Broccolini/Baby Broccoli  0.00198   \n",
       "4            Baby Cucumbers                        Hass Avocados  0.00103   \n",
       "\n",
       "  Confidence     Lift  \n",
       "0    0.27941  2.26811  \n",
       "1    0.25411  2.06274  \n",
       "2    0.32820  33.6140  \n",
       "3    0.36813  2.39230  \n",
       "4    0.22707  14.0823  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_suggestions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clementines appeared in 0.00131 of the total transactions (0.13%)\n",
    "- The likelihood of someone buying Apples when they purchase Clementines is 0.32 (32%)\n",
    "- Past transanctions shows that people are 33 times more likely to buy Apples and Clementines compared to just buying Apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedent</th>\n",
       "      <th>Consequents</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Organic Tomato Cluster</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>0.00102</td>\n",
       "      <td>0.20763</td>\n",
       "      <td>2.63702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Organic Strawberries</td>\n",
       "      <td>Large Yellow Flesh Nectarine</td>\n",
       "      <td>0.00117</td>\n",
       "      <td>0.23448</td>\n",
       "      <td>2.72036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Organic Cucumber</td>\n",
       "      <td>Organic Hass Avocado</td>\n",
       "      <td>0.00570</td>\n",
       "      <td>0.21740</td>\n",
       "      <td>3.12255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Limes</td>\n",
       "      <td>Bag of Organic Bananas</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.20703</td>\n",
       "      <td>2.62933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Strawberries</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.31924</td>\n",
       "      <td>2.07461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Antecedent                   Consequents  Support Confidence  \\\n",
       "208  Organic Tomato Cluster        Bag of Organic Bananas  0.00102    0.20763   \n",
       "84     Organic Strawberries  Large Yellow Flesh Nectarine  0.00117    0.23448   \n",
       "120        Organic Cucumber          Organic Hass Avocado  0.00570    0.21740   \n",
       "184                   Limes        Bag of Organic Bananas  0.00141    0.20703   \n",
       "326                  Banana                  Strawberries  0.00143    0.31924   \n",
       "\n",
       "        Lift  \n",
       "208  2.63702  \n",
       "84   2.72036  \n",
       "120  3.12255  \n",
       "184  2.62933  \n",
       "326  2.07461  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_suggestions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 421 entries, 0 to 420\n",
      "Data columns (total 5 columns):\n",
      "Antecedent     421 non-null object\n",
      "Consequents    421 non-null object\n",
      "Support        421 non-null object\n",
      "Confidence     421 non-null object\n",
      "Lift           421 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 16.6+ KB\n"
     ]
    }
   ],
   "source": [
    "product_suggestions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_df = full_df.groupby(['user_id', 'product_name', 'product_id'])['product_id'].agg('count').to_frame('purchase_count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of unique users\n",
    "users = list(np.sort(collaborative_df['user_id'].unique()))\n",
    "# get a list of unique products\n",
    "products = list(collaborative_df['product_id'].unique())\n",
    "# get a list of purchase count\n",
    "purchase_count = list(collaborative_df['purchase_count'])\n",
    "\n",
    "# get the row indices\n",
    "cols = collaborative_df['user_id'].astype('category', CategoricalDtype(categories = users)).cat.codes\n",
    "# get the column indices\n",
    "rows = collaborative_df['product_id'].astype('category', CategoricalDtype(categories = products)).cat.codes\n",
    "\n",
    "collaborative_sparse = sparse.csr_matrix((purchase_count, (rows, cols)), shape = (len(products), len(users)))\n",
    "\n",
    "# purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<48422x206209 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 13266179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborative_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 206209 customers with 48422 items. lets check our sparcity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mas = MaxAbsScaler()\n",
    "collaborative_sparse = mas.fit_transform(collaborative_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.\n",
    "\n",
    "MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparcity is 99.86713961292403\n"
     ]
    }
   ],
   "source": [
    "# get the number of possible interactions\n",
    "matrix_size = collaborative_sparse.shape[0] * collaborative_sparse.shape[1]\n",
    "\n",
    "# number of actual interactions\n",
    "purchase_num = len(collaborative_sparse.nonzero()[0])\n",
    "print('Sparcity is {}'.format(100*(1-(purchase_num/matrix_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this to work we the maximum sparcity should be about 99.5% we are 0.3% above it, this may affect our result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# closest to the recommendations in the slides\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.01, iterations=20)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this is quite good\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.01, iterations=25)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this one also not so bad\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.001, iterations=25)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this one also quite close to the power point recommendations\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0525b13df881482a8056ffceba6b8f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# closest to the power point\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=350, regularization=0.1, iterations=40)\n",
    "alpha_val = 40\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)\n",
    "user_items = data_conf.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, model, fitted, user):\n",
    "    recommendations = model.recommend(user, fitted, N = 4, filter_already_liked_items = False)\n",
    "    product_dict = dict(zip(full_df.product_id, full_df.product_name))\n",
    "    \n",
    "    print('Recommended items for user {} are: \\n'.format(user))\n",
    "    for i in recommendations:\n",
    "        print(i[0], product_dict.get(i[0]), i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 1 are: \n",
      "\n",
      "33409 Sugar Free Cherry Gelatin Dessert 1.2291799\n",
      "13517 Whole Wheat Bread 1.2027234\n",
      "24332 Carrot Ginger With Coconut Soup 1.1276637\n",
      "20493 All Natural Chocolate Hemp 1.0903381\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 2 are: \n",
      "\n",
      "426 2nd Foods Bananas 1.2574742\n",
      "20319 Water Chestnuts, Sliced 1.1400969\n",
      "45944 Devils Food Cake Mix 1.134639\n",
      "39521 Premium Potatoes Organic Fingerling Medley 1.0907922\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 5 are: \n",
      "\n",
      "21353 Shredded Pizza Cheese 0.72977406\n",
      "24342 Big Chewy Peanut Butter Chocolate Chip Granola 0.7140698\n",
      "46544 Mild Tikka Curry Paste 0.70919603\n",
      "22362 Original Rice Krispies Treats 0.7051064\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 7 are: \n",
      "\n",
      "17338 Vanilla Swiss Almond Ice Cream 0.9824994\n",
      "30924 Cherry Real Italian Ice 0.9820426\n",
      "33502 Double Cheese Baked Snack Mix 0.97929156\n",
      "33275 Aloe Vera Soap Bar 0.97042596\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 264 are: \n",
      "\n",
      "12835 Good Start® Gentle for Supplementing Powder Infant Formula 1.016046\n",
      "24231 Tahitian Vanilla Bean Gelato 0.99917996\n",
      "21353 Shredded Pizza Cheese 0.9966327\n",
      "20608 Grapeseed Oil 0.9963527\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 3754 are: \n",
      "\n",
      "33502 Double Cheese Baked Snack Mix 1.2523721\n",
      "29408 Mean Beans Spicy Green Bean Pickles 1.1054409\n",
      "45339 Men's Refresh Dandruff Shampoo 1.0747702\n",
      "11114 Whole Poppy Seed 1.0670176\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(collaborative_sparse, model, user_items, 3754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Codes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Bayesian Personalized Ranking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model2 = implicit.bpr.BayesianPersonalizedRanking(factors=100, regularization=0.01, iterations=25)\n",
    "alpha_val = 40\n",
    "data_conf2 = (collaborative_sparse * alpha_val).astype('double')\n",
    "model2.fit(data_conf2)\n",
    "user_items2 = data_conf2.T.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 264)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model2, user_items2, 3754)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Logistic Matrix Factorization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model3 = implicit.lmf.LogisticMatrixFactorization(factors=100, learning_rate = 0.01, regularization=0.01, iterations=25)\n",
    "alpha_val = 40\n",
    "data_conf3 = (collaborative_sparse * alpha_val).astype('double')\n",
    "model3.fit(data_conf3)\n",
    "user_items3 = data_conf3.T.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 264)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_recommendations(collaborative_sparse, model3, user_items3, 3754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha_val = 600\n",
    "data_conf = (collaborative_sparse * alpha_val).astype('double')\n",
    "model.fit(data_conf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_items = data_conf.T.tocsr()\n",
    "recommendations = model.recommend(1, user_items, filter_already_liked_items = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mydict = dict(zip(collaborative_df.product_id, collaborative_df.product_name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in recommendations:\n",
    "    print(mydict.get(i[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "collaborative_df[collaborative_df['user_id'] == 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mydict.get(26303)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "recommendations[1][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_df[full_df['user_id'] == 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_df[full_df['product_id'] == 18133]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Similar users\n",
    "\n",
    "model.recommend(2, product_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# similar products\n",
    "\n",
    "model.similar_items(1992, 7)\n",
    "\n",
    "# find 7 most similar items to product 3847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test our recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our recommender system"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_train(ratings, pct_test = 0.2):\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "\n",
    "parameters: \n",
    "\n",
    "ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "\n",
    "pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "training set for later comparison to the test set, which contains all of the original ratings. \n",
    "\n",
    "returns:\n",
    "\n",
    "training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "that originally had interaction set back to zero.\n",
    "\n",
    "test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "compares with the actual interactions.\n",
    "\n",
    "user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "This will be necessary later when evaluating the performance via AUC."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "product_train, product_test, product_users_altered = make_train(collaborative_sparse, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((product_train*alpha).astype('double'), \n",
    "                                                          factors=20, \n",
    "                                                          regularization = 0.1, \n",
    "                                                         iterations = 50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha = 15\n",
    "users_vecs, products_vecs = implicit.alternating_least_squares((product_train * alpha).astype('double'), factors = 20, regularization = 0.1, iterations = 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple function will output the area under the curve using sklearn's metrics. \n",
    "\n",
    "parameters:\n",
    "\n",
    "- predictions: your prediction output\n",
    "\n",
    "- test: the actual target result you are comparing to\n",
    "\n",
    "returns:\n",
    "\n",
    "- AUC (area under the Receiver Operating Characterisic curve)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def auc_score(predictions, test):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "\n",
    "parameters:\n",
    "\n",
    "training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "user/item interactions are reset to zero to hide them from the model \n",
    "\n",
    "predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "\n",
    "altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "\n",
    "test_set - The test set constucted earlier from make_train function\n",
    "\n",
    "\n",
    "\n",
    "returns:\n",
    "\n",
    "The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "   \n",
    "    store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = [] # To store popular AUC scores\n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users: # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user,:].toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training \n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC rounded to three decimal places for both test and popularity benchmark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calc_mean_auc(product_train, product_users_altered, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], product_test)\n",
    "# AUC for our recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def implicit_weighted_ALS(training_set, lambda_val = 0.1, alpha = 40, iterations = 10, rank_size = 20, seed = 0):\n",
    "\n",
    "    # first set up our confidence matrix\n",
    "    \n",
    "    conf = (alpha*training_set) # To allow the matrix to stay sparse, I will add one later when each row is taken \n",
    "                                # and converted to dense. \n",
    "    num_user = conf.shape[0]\n",
    "    num_item = conf.shape[1] # Get the size of our original ratings matrix, m x n\n",
    "    \n",
    "    # initialize our X/Y feature vectors randomly with a set seed\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size))) # Random numbers in a m x rank shape\n",
    "    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size))) # Normally this would be rank x n but we can \n",
    "                                                                 # transpose at the end. Makes calculation more simple.\n",
    "    X_eye = sparse.eye(num_user)\n",
    "    Y_eye = sparse.eye(num_item)\n",
    "    lambda_eye = lambda_val * sparse.eye(rank_size) # Our regularization term lambda*I. \n",
    "    \n",
    "    # We can compute this before iteration starts. \n",
    "    \n",
    "    # Begin iterations\n",
    "   \n",
    "    for iter_step in range(iterations): # Iterate back and forth between solving X given fixed Y and vice versa\n",
    "        # Compute yTy and xTx at beginning of each iteration to save computing time\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "        # Being iteration to solve for X based on fixed Y\n",
    "        for u in range(num_user):\n",
    "            conf_samp = conf[u,:].toarray() # Grab user row from confidence matrix and convert to dense\n",
    "            pref = conf_samp.copy() \n",
    "            pref[pref != 0] = 1 # Create binarized preference vector \n",
    "            CuI = sparse.diags(conf_samp, [0]) # Get Cu - I term, don't need to subtract 1 since we never added it \n",
    "            yTCuIY = Y.T.dot(CuI).dot(Y) # This is the yT(Cu-I)Y term \n",
    "            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) # This is the yTCuPu term, where we add the eye back in\n",
    "                                                      # Cu - I + I = Cu\n",
    "            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu) \n",
    "            # Solve for Xu = ((yTy + yT(Cu-I)Y + lambda*I)^-1)yTCuPu, equation 4 from the paper  \n",
    "        # Begin iteration to solve for Y based on fixed X \n",
    "        for i in range(num_item):\n",
    "            conf_samp = conf[:,i].T.toarray() # transpose to get it in row format and convert to dense\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1 # Create binarized preference vector\n",
    "            CiI = sparse.diags(conf_samp, [0]) # Get Ci - I term, don't need to subtract 1 since we never added it\n",
    "            xTCiIX = X.T.dot(CiI).dot(X) # This is the xT(Cu-I)X term\n",
    "            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) # This is the xTCiPi term\n",
    "            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)\n",
    "            # Solve for Yi = ((xTx + xT(Cu-I)X) + lambda*I)^-1)xTCiPi, equation 5 from the paper\n",
    "    # End iterations\n",
    "    return X, Y.T # Transpose at the end to make up for not being transposed at the beginning. \n",
    "                         # Y needs to be rank x n. Keep these as separate matrices for scale reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit weighted ALS taken from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit\n",
    "feedback based collaborative filtering. \n",
    "\n",
    "parameters:\n",
    "\n",
    "training_set - Our matrix of ratings with shape m x n, where m is the number of users and n is the number of items.\n",
    "Should be a sparse csr matrix to save space. \n",
    "\n",
    "lambda_val - Used for regularization during alternating least squares. Increasing this value may increase bias\n",
    "but decrease variance. Default is 0.1. \n",
    "\n",
    "alpha - The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. \n",
    "The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between\n",
    "various ratings.\n",
    "\n",
    "iterations - The number of times to alternate between both user feature vector and item feature vector in\n",
    "alternating least squares. More iterations will allow better convergence at the cost of increased computation. \n",
    "The authors found 10 iterations was sufficient, but more may be required to converge. \n",
    "\n",
    "rank_size - The number of latent features in the user/item feature vectors. The paper recommends varying this \n",
    "between 20-200. Increasing the number of features may overfit but could reduce bias. \n",
    "\n",
    "seed - Set the seed for reproducible results\n",
    "\n",
    "returns:\n",
    "\n",
    "The feature vectors for users and items. The dot product of these feature vectors should give you the expected \n",
    "\"rating\" at each point in your original matrix. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_vecs, item_vecs = implicit_weighted_ALS(product_train, lambda_val = 0.1, alpha = 15, iterations = 1,\n",
    "                                            rank_size = 20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_vecs[0,:].dot(item_vecs).toarray()[0,:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_df.groupby(['user_id', 'product_name', 'product_id'])['product_name'].agg('count').to_frame('purchase_count').reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
